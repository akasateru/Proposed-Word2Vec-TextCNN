{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-27 15:33:14.930263: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-27 15:33:15.484438: W tensorflow/tsl/platform/default/dso_loader.cc:66] Could not load dynamic library 'libnvinfer.so.8'; dlerror: libnvinfer.so.8: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:\n",
      "2022-11-27 15:33:15.484485: W tensorflow/tsl/platform/default/dso_loader.cc:66] Could not load dynamic library 'libnvinfer_plugin.so.8'; dlerror: libnvinfer_plugin.so.8: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:\n",
      "2022-11-27 15:33:15.484489: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# import os\n",
    "# os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"1\"\n",
    "from sklearn import metrics\n",
    "from tensorflow import keras\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import gensim.downloader\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import string\n",
    "import csv \n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "import datetime\n",
    "t_delta = datetime.timedelta(hours=9)\n",
    "JST = datetime.timezone(t_delta, 'JST')\n",
    "now = datetime.datetime.now(JST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "SAVED_MODEL = \"../model/Proposed-Word2Vec-TextCNN_\"+str(now.strftime('%Y%m%d%H%M%S'))\n",
    "THRESHOLD = 0.05\n",
    "MAXLEN_GET_PSEUDO = 3000\n",
    "EPOCH = 5\n",
    "BATCH_SIZE = 64\n",
    "MAX_LEN = 128\n",
    "FILTERS = 100\n",
    "FILTER_SIZES = [3,4,5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../model/Proposed-Word2Vec-TextCNN_20221127153316\n"
     ]
    }
   ],
   "source": [
    "print(SAVED_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 前処理\n",
    "def preprocessing(text):\n",
    "    # 括弧内文章の削除\n",
    "    text = re.sub(r'\\(.*\\)',' ',text)\n",
    "    text = re.sub(r'\\[.*\\]',' ',text)\n",
    "    text = re.sub(r'\\<.*\\>',' ',text)\n",
    "    text = re.sub(r'\\{.*\\}',' ',text)\n",
    "    # 記号文字の削除\n",
    "    text = text.translate(str.maketrans('','',string.punctuation))\n",
    "    # スペースの調整\n",
    "    text = re.sub(r'\\s+',' ',text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:00<00:00, 19380.93it/s]\n"
     ]
    }
   ],
   "source": [
    "# 20 newsgroups datasets\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "newsgroups = fetch_20newsgroups(subset=\"all\")\n",
    "newsgroups_datasets = list()\n",
    "\n",
    "# example ------------------------------------------------\n",
    "for texts in tqdm(newsgroups.data[:1000]):\n",
    "  texts = texts.split(\"\\n\\n\")\n",
    "  texts = \" \".join(texts[1:])\n",
    "  newsgroups_datasets.append(preprocessing(texts))\n",
    "# --------------------------------------------------------\n",
    "\n",
    "# for texts in tqdm(newsgroups.data):\n",
    "#   texts = texts.split(\"\\n\\n\")\n",
    "#   texts = \" \".join(texts[1:])\n",
    "#   newsgroups_datasets.append(preprocessing(texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:00<00:00, 52032.06it/s]\n"
     ]
    }
   ],
   "source": [
    "# yahoo topic datasets\n",
    "with open('../data/topic/train_pu_half_v0.txt','r',encoding='utf-8') as f:\n",
    "    texts_v0 = f.read()\n",
    "with open('../data/topic/train_pu_half_v1.txt','r',encoding='utf-8') as f:\n",
    "    texts_v1 = f.read()\n",
    "texts = texts_v0 + texts_v1\n",
    "topic_datasets = list()\n",
    "\n",
    "# example ----------------------------------------------\n",
    "for label_text in tqdm(texts.splitlines()[:1000]):\n",
    "  _, text = label_text.split(\"\\t\")\n",
    "  topic_datasets.append(preprocessing(text))\n",
    "# -------------------------------------------------------\n",
    "\n",
    "# for label_text in tqdm(texts.splitlines()):\n",
    "#   _, text = label_text.split(\"\\t\")\n",
    "#   topic_datasets.append(preprocessing(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:00<00:00, 24637.45it/s]\n"
     ]
    }
   ],
   "source": [
    "# reuters datasets\n",
    "with open(\"../data/reuter/sourceall.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "  reuter = f.read().split(\"\\n\")[:-1]\n",
    "\n",
    "# example -----------------------------------\n",
    "reuter = reuter[:1000]\n",
    "# -------------------------------------------\n",
    "\n",
    "reuters_datasets = list()\n",
    "for label_text in tqdm(reuter):\n",
    "  _, text = label_text.split(\"\\t\")\n",
    "  reuters_datasets.append(preprocessing(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:00<00:00, 63805.28it/s]\n"
     ]
    }
   ],
   "source": [
    "# dbpedia datasets train\n",
    "with open('../data/dbpedia_csv/train.csv','r',encoding='utf-8') as f:\n",
    "    reader = [r for r in csv.reader(f)]\n",
    "    \n",
    "# example -------------------\n",
    "reader = reader[:1000]\n",
    "#----------------------------\n",
    "\n",
    "dbpedia_train_datasets = list()\n",
    "for _, auth, text in tqdm(reader):\n",
    "    text = text.replace(auth,'')\n",
    "    dbpedia_train_datasets.append(preprocessing(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dbpedia classes\n",
    "with open(\"../data/dbpedia_csv/classes.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "  classes = f.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_texts = newsgroups_datasets + topic_datasets + reuters_datasets + dbpedia_train_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choice method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec = gensim.downloader.load('word2vec-google-news-300')\n",
    "\n",
    "def w2v_avg_vector(sentence):\n",
    "  vector = np.zeros((300,), dtype=\"float32\")\n",
    "  count = 0\n",
    "  for word in sentence.split():\n",
    "    try:\n",
    "      vector = np.add(vector, word2vec[word])\n",
    "      count += 1\n",
    "    except:\n",
    "      pass\n",
    "  if count > 0:\n",
    "    vector = np.divide(vector, len(word))\n",
    "  return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes_vector = list()\n",
    "for cls in classes:\n",
    "  classes_vector.append(w2v_avg_vector(cls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4000/4000 [00:01<00:00, 2802.60it/s]\n"
     ]
    }
   ],
   "source": [
    "diff_datasets = {i:[] for i in range(len(classes))}\n",
    "for texts in tqdm(datasets_texts):\n",
    "  texts_vector = w2v_avg_vector(texts)\n",
    "  similarity = cosine_similarity([texts_vector], classes_vector)[0]\n",
    "  sim_argsorted = np.argsort(similarity)\n",
    "  diff = similarity[sim_argsorted[-1]] - similarity[sim_argsorted[-2]]\n",
    "  if diff > THRESHOLD:\n",
    "    diff_datasets[sim_argsorted[-1]].append((similarity[sim_argsorted[-1]], texts))\n",
    "\n",
    "pseudo_texts = list()\n",
    "pseudo_labels = list()\n",
    "for i in range(len(classes)):\n",
    "  sorted_diff_data = sorted(diff_datasets[i], reverse=True)[:MAXLEN_GET_PSEUDO]\n",
    "  pseudo_texts.extend([i[1] for i in sorted_diff_data])\n",
    "  pseudo_labels.extend([i]*len(sorted_diff_data[:MAXLEN_GET_PSEUDO]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of all selected data\n",
      "Com. : 273\n",
      "Edu. : 34\n",
      "Art. : 1\n",
      "Ath. : 2\n",
      "Off. : 174\n",
      "Mea. : 210\n",
      "Bui. : 7\n",
      "Nat. : 53\n",
      "Vil. : 1\n",
      "Ani. : 11\n",
      "Pla. : 6\n",
      "Alb. : 24\n",
      "Fil. : 30\n",
      "Wri. : 18\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of all selected data\")\n",
    "for i in diff_datasets:\n",
    "  print(classes[i][:3]+\". : \"+str(len(diff_datasets[i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = keras.preprocessing.text.Tokenizer()\n",
    "tokenizer.fit_on_texts(pseudo_texts)\n",
    "sequences = tokenizer.texts_to_sequences(pseudo_texts)\n",
    "x_train = keras.preprocessing.sequence.pad_sequences(sequences, maxlen=MAX_LEN, padding='post')\n",
    "y_train = np.array(keras.utils.to_categorical(pseudo_labels))\n",
    "\n",
    "embedding_matrix = np.zeros((max(tokenizer.word_index.values())+1, word2vec.vector_size))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    if word in word2vec:\n",
    "        embedding_matrix[i] = word2vec[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = keras.layers.Input(shape=(MAX_LEN),dtype='float32')\n",
    "embedding = keras.layers.Embedding(input_dim=len(embedding_matrix),output_dim=len(embedding_matrix[0]),weights=[embedding_matrix],trainable=False, mask_zero=True,name='embedding')(inputs)\n",
    "conv1 = keras.layers.Conv1D(filters=FILTERS, kernel_size=FILTER_SIZES[0], kernel_initializer='normal', activation='relu')(embedding)\n",
    "conv2 = keras.layers.Conv1D(filters=FILTERS, kernel_size=FILTER_SIZES[1], kernel_initializer='normal', activation='relu')(embedding)\n",
    "conv3 = keras.layers.Conv1D(filters=FILTERS, kernel_size=FILTER_SIZES[2], kernel_initializer='normal', activation='relu')(embedding)\n",
    "pool1 = keras.layers.MaxPooling1D(pool_size=int(conv1.shape[1]),strides=1)(conv1)\n",
    "pool2 = keras.layers.MaxPooling1D(pool_size=int(conv2.shape[1]),strides=1)(conv2)\n",
    "pool3 = keras.layers.MaxPooling1D(pool_size=int(conv3.shape[1]),strides=1)(conv3)\n",
    "x = keras.layers.concatenate([pool1, pool2, pool3])\n",
    "x = keras.layers.Flatten()(x)\n",
    "x = keras.layers.Dropout(0.5)(x)\n",
    "x = keras.layers.Dense(100,activation='relu')(x)\n",
    "output = keras.layers.Dense(units=14, activation='softmax')(x)\n",
    "model = keras.models.Model(inputs, output)\n",
    "opt = keras.optimizers.Adam(learning_rate=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-07, amsgrad=True)\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-27 15:33:59.845788: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:421] Loaded runtime CuDNN library: 8.4.0 but source was compiled with: 8.6.0.  CuDNN library needs to have matching major version and equal or higher minor version. If using a binary install, upgrade your CuDNN library.  If building from sources, make sure the library loaded at runtime is compatible with the version specified during compile configuration.\n",
      "2022-11-27 15:33:59.846348: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at conv_ops.cc:1060 : UNIMPLEMENTED: DNN library is not found.\n",
      "2022-11-27 15:33:59.847275: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:421] Loaded runtime CuDNN library: 8.4.0 but source was compiled with: 8.6.0.  CuDNN library needs to have matching major version and equal or higher minor version. If using a binary install, upgrade your CuDNN library.  If building from sources, make sure the library loaded at runtime is compatible with the version specified during compile configuration.\n",
      "2022-11-27 15:33:59.847629: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at conv_ops.cc:1060 : UNIMPLEMENTED: DNN library is not found.\n",
      "2022-11-27 15:33:59.848586: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:421] Loaded runtime CuDNN library: 8.4.0 but source was compiled with: 8.6.0.  CuDNN library needs to have matching major version and equal or higher minor version. If using a binary install, upgrade your CuDNN library.  If building from sources, make sure the library loaded at runtime is compatible with the version specified during compile configuration.\n",
      "2022-11-27 15:33:59.849025: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at conv_ops.cc:1060 : UNIMPLEMENTED: DNN library is not found.\n"
     ]
    },
    {
     "ename": "UnimplementedError",
     "evalue": "Graph execution error:\n\nDetected at node 'model/conv1d/Conv1D' defined at (most recent call last):\n    File \"/home/dbl/.pyenv/versions/3.9.1/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"/home/dbl/.pyenv/versions/3.9.1/lib/python3.9/runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"/home/dbl/.pyenv/versions/3.9.1/lib/python3.9/site-packages/ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"/home/dbl/.pyenv/versions/3.9.1/lib/python3.9/site-packages/traitlets/config/application.py\", line 982, in launch_instance\n      app.start()\n    File \"/home/dbl/.pyenv/versions/3.9.1/lib/python3.9/site-packages/ipykernel/kernelapp.py\", line 712, in start\n      self.io_loop.start()\n    File \"/home/dbl/.pyenv/versions/3.9.1/lib/python3.9/site-packages/tornado/platform/asyncio.py\", line 215, in start\n      self.asyncio_loop.run_forever()\n    File \"/home/dbl/.pyenv/versions/3.9.1/lib/python3.9/asyncio/base_events.py\", line 596, in run_forever\n      self._run_once()\n    File \"/home/dbl/.pyenv/versions/3.9.1/lib/python3.9/asyncio/base_events.py\", line 1890, in _run_once\n      handle._run()\n    File \"/home/dbl/.pyenv/versions/3.9.1/lib/python3.9/asyncio/events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/home/dbl/.pyenv/versions/3.9.1/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 510, in dispatch_queue\n      await self.process_one()\n    File \"/home/dbl/.pyenv/versions/3.9.1/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 499, in process_one\n      await dispatch(*args)\n    File \"/home/dbl/.pyenv/versions/3.9.1/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 406, in dispatch_shell\n      await result\n    File \"/home/dbl/.pyenv/versions/3.9.1/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 730, in execute_request\n      reply_content = await reply_content\n    File \"/home/dbl/.pyenv/versions/3.9.1/lib/python3.9/site-packages/ipykernel/ipkernel.py\", line 383, in do_execute\n      res = shell.run_cell(\n    File \"/home/dbl/.pyenv/versions/3.9.1/lib/python3.9/site-packages/ipykernel/zmqshell.py\", line 528, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"/home/dbl/.pyenv/versions/3.9.1/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 2940, in run_cell\n      result = self._run_cell(\n    File \"/home/dbl/.pyenv/versions/3.9.1/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 2995, in _run_cell\n      return runner(coro)\n    File \"/home/dbl/.pyenv/versions/3.9.1/lib/python3.9/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/home/dbl/.pyenv/versions/3.9.1/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3194, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"/home/dbl/.pyenv/versions/3.9.1/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3373, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"/home/dbl/.pyenv/versions/3.9.1/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3433, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"/tmp/ipykernel_21048/3610004364.py\", line 2, in <module>\n      result = model.fit(x=x_train,\n    File \"/home/dbl/.pyenv/versions/3.9.1/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/dbl/.pyenv/versions/3.9.1/lib/python3.9/site-packages/keras/engine/training.py\", line 1680, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"/home/dbl/.pyenv/versions/3.9.1/lib/python3.9/site-packages/keras/engine/training.py\", line 1279, in train_function\n      return step_function(self, iterator)\n    File \"/home/dbl/.pyenv/versions/3.9.1/lib/python3.9/site-packages/keras/engine/training.py\", line 1263, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/dbl/.pyenv/versions/3.9.1/lib/python3.9/site-packages/keras/engine/training.py\", line 1244, in run_step\n      outputs = model.train_step(data)\n    File \"/home/dbl/.pyenv/versions/3.9.1/lib/python3.9/site-packages/keras/engine/training.py\", line 1045, in train_step\n      y_pred = self(x, training=True)\n    File \"/home/dbl/.pyenv/versions/3.9.1/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/dbl/.pyenv/versions/3.9.1/lib/python3.9/site-packages/keras/engine/training.py\", line 557, in __call__\n      return super().__call__(*args, **kwargs)\n    File \"/home/dbl/.pyenv/versions/3.9.1/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/dbl/.pyenv/versions/3.9.1/lib/python3.9/site-packages/keras/engine/base_layer.py\", line 1132, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/home/dbl/.pyenv/versions/3.9.1/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/dbl/.pyenv/versions/3.9.1/lib/python3.9/site-packages/keras/engine/functional.py\", line 511, in call\n      return self._run_internal_graph(inputs, training=training, mask=mask)\n    File \"/home/dbl/.pyenv/versions/3.9.1/lib/python3.9/site-packages/keras/engine/functional.py\", line 668, in _run_internal_graph\n      outputs = node.layer(*args, **kwargs)\n    File \"/home/dbl/.pyenv/versions/3.9.1/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/dbl/.pyenv/versions/3.9.1/lib/python3.9/site-packages/keras/engine/base_layer.py\", line 1132, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/home/dbl/.pyenv/versions/3.9.1/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/dbl/.pyenv/versions/3.9.1/lib/python3.9/site-packages/keras/layers/convolutional/base_conv.py\", line 283, in call\n      outputs = self.convolution_op(inputs, self.kernel)\n    File \"/home/dbl/.pyenv/versions/3.9.1/lib/python3.9/site-packages/keras/layers/convolutional/base_conv.py\", line 255, in convolution_op\n      return tf.nn.convolution(\nNode: 'model/conv1d/Conv1D'\nDNN library is not found.\n\t [[{{node model/conv1d/Conv1D}}]] [Op:__inference_train_function_1860]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnimplementedError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [18], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# モデルの学習\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m result \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(x\u001b[39m=\u001b[39;49mx_train,\n\u001b[1;32m      3\u001b[0m                    y\u001b[39m=\u001b[39;49my_train,\n\u001b[1;32m      4\u001b[0m                    epochs\u001b[39m=\u001b[39;49mEPOCH,\n\u001b[1;32m      5\u001b[0m                    batch_size\u001b[39m=\u001b[39;49mBATCH_SIZE,\n\u001b[1;32m      6\u001b[0m                    validation_data\u001b[39m=\u001b[39;49m(x_train,y_train))\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.1/lib/python3.9/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.1/lib/python3.9/site-packages/tensorflow/python/eager/execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 52\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39mTFE_Py_Execute(ctx\u001b[39m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     53\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     54\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     55\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mUnimplementedError\u001b[0m: Graph execution error:\n\nDetected at node 'model/conv1d/Conv1D' defined at (most recent call last):\n    File \"/home/dbl/.pyenv/versions/3.9.1/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"/home/dbl/.pyenv/versions/3.9.1/lib/python3.9/runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"/home/dbl/.pyenv/versions/3.9.1/lib/python3.9/site-packages/ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"/home/dbl/.pyenv/versions/3.9.1/lib/python3.9/site-packages/traitlets/config/application.py\", line 982, in launch_instance\n      app.start()\n    File \"/home/dbl/.pyenv/versions/3.9.1/lib/python3.9/site-packages/ipykernel/kernelapp.py\", line 712, in start\n      self.io_loop.start()\n    File \"/home/dbl/.pyenv/versions/3.9.1/lib/python3.9/site-packages/tornado/platform/asyncio.py\", line 215, in start\n      self.asyncio_loop.run_forever()\n    File \"/home/dbl/.pyenv/versions/3.9.1/lib/python3.9/asyncio/base_events.py\", line 596, in run_forever\n      self._run_once()\n    File \"/home/dbl/.pyenv/versions/3.9.1/lib/python3.9/asyncio/base_events.py\", line 1890, in _run_once\n      handle._run()\n    File \"/home/dbl/.pyenv/versions/3.9.1/lib/python3.9/asyncio/events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/home/dbl/.pyenv/versions/3.9.1/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 510, in dispatch_queue\n      await self.process_one()\n    File \"/home/dbl/.pyenv/versions/3.9.1/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 499, in process_one\n      await dispatch(*args)\n    File \"/home/dbl/.pyenv/versions/3.9.1/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 406, in dispatch_shell\n      await result\n    File \"/home/dbl/.pyenv/versions/3.9.1/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 730, in execute_request\n      reply_content = await reply_content\n    File \"/home/dbl/.pyenv/versions/3.9.1/lib/python3.9/site-packages/ipykernel/ipkernel.py\", line 383, in do_execute\n      res = shell.run_cell(\n    File \"/home/dbl/.pyenv/versions/3.9.1/lib/python3.9/site-packages/ipykernel/zmqshell.py\", line 528, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"/home/dbl/.pyenv/versions/3.9.1/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 2940, in run_cell\n      result = self._run_cell(\n    File \"/home/dbl/.pyenv/versions/3.9.1/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 2995, in _run_cell\n      return runner(coro)\n    File \"/home/dbl/.pyenv/versions/3.9.1/lib/python3.9/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/home/dbl/.pyenv/versions/3.9.1/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3194, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"/home/dbl/.pyenv/versions/3.9.1/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3373, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"/home/dbl/.pyenv/versions/3.9.1/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3433, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"/tmp/ipykernel_21048/3610004364.py\", line 2, in <module>\n      result = model.fit(x=x_train,\n    File \"/home/dbl/.pyenv/versions/3.9.1/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/dbl/.pyenv/versions/3.9.1/lib/python3.9/site-packages/keras/engine/training.py\", line 1680, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"/home/dbl/.pyenv/versions/3.9.1/lib/python3.9/site-packages/keras/engine/training.py\", line 1279, in train_function\n      return step_function(self, iterator)\n    File \"/home/dbl/.pyenv/versions/3.9.1/lib/python3.9/site-packages/keras/engine/training.py\", line 1263, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/dbl/.pyenv/versions/3.9.1/lib/python3.9/site-packages/keras/engine/training.py\", line 1244, in run_step\n      outputs = model.train_step(data)\n    File \"/home/dbl/.pyenv/versions/3.9.1/lib/python3.9/site-packages/keras/engine/training.py\", line 1045, in train_step\n      y_pred = self(x, training=True)\n    File \"/home/dbl/.pyenv/versions/3.9.1/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/dbl/.pyenv/versions/3.9.1/lib/python3.9/site-packages/keras/engine/training.py\", line 557, in __call__\n      return super().__call__(*args, **kwargs)\n    File \"/home/dbl/.pyenv/versions/3.9.1/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/dbl/.pyenv/versions/3.9.1/lib/python3.9/site-packages/keras/engine/base_layer.py\", line 1132, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/home/dbl/.pyenv/versions/3.9.1/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/dbl/.pyenv/versions/3.9.1/lib/python3.9/site-packages/keras/engine/functional.py\", line 511, in call\n      return self._run_internal_graph(inputs, training=training, mask=mask)\n    File \"/home/dbl/.pyenv/versions/3.9.1/lib/python3.9/site-packages/keras/engine/functional.py\", line 668, in _run_internal_graph\n      outputs = node.layer(*args, **kwargs)\n    File \"/home/dbl/.pyenv/versions/3.9.1/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/dbl/.pyenv/versions/3.9.1/lib/python3.9/site-packages/keras/engine/base_layer.py\", line 1132, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/home/dbl/.pyenv/versions/3.9.1/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/dbl/.pyenv/versions/3.9.1/lib/python3.9/site-packages/keras/layers/convolutional/base_conv.py\", line 283, in call\n      outputs = self.convolution_op(inputs, self.kernel)\n    File \"/home/dbl/.pyenv/versions/3.9.1/lib/python3.9/site-packages/keras/layers/convolutional/base_conv.py\", line 255, in convolution_op\n      return tf.nn.convolution(\nNode: 'model/conv1d/Conv1D'\nDNN library is not found.\n\t [[{{node model/conv1d/Conv1D}}]] [Op:__inference_train_function_1860]"
     ]
    }
   ],
   "source": [
    "# モデルの学習\n",
    "result = model.fit(x=x_train,\n",
    "                   y=y_train,\n",
    "                   epochs=EPOCH,\n",
    "                   batch_size=BATCH_SIZE,\n",
    "                   validation_data=(x_train,y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(SAVED_MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model(SAVED_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load test data\n",
    "# dbpedia datasets train\n",
    "with open('../data/dbpedia_csv/test.csv','r',encoding='utf-8') as f:\n",
    "    reader = [r for r in csv.reader(f)]\n",
    "    \n",
    "# example -------------------\n",
    "import random\n",
    "reader = random.sample(reader, 1000)\n",
    "#----------------------------\n",
    "\n",
    "test_texts = list()\n",
    "test_labels = list()\n",
    "for labels, auth, text in tqdm(reader):\n",
    "    text = text.replace(auth,'')\n",
    "    test_texts.append(preprocessing(text))\n",
    "    test_labels.append(int(labels)-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = keras.preprocessing.text.Tokenizer()\n",
    "tokenizer.fit_on_texts(test_texts)\n",
    "sequences = tokenizer.texts_to_sequences(test_texts)\n",
    "x_test = keras.preprocessing.sequence.pad_sequences(sequences, maxlen=MAX_LEN, padding='post')\n",
    "y_test = np.array(keras.utils.to_categorical(test_labels))\n",
    "\n",
    "embedding_matrix = np.zeros((max(tokenizer.word_index.values())+1, word2vec.vector_size))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    if word in word2vec:\n",
    "        embedding_matrix[i] = word2vec[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.embedding = keras.layers.Embedding(input_dim=len(embedding_matrix),output_dim=len(embedding_matrix[0]),weights=[embedding_matrix],trainable=False, mask_zero=True,name='embedding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "y_test = [np.argmax(i) for i in y_test]\n",
    "y_pred = [np.argmax(i) for i in pred]\n",
    "\n",
    "target_names = [c[:3]+\".\" for c in classes]\n",
    "\n",
    "rep = metrics.classification_report(y_test, y_pred, target_names=target_names, digits=3)\n",
    "print(rep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_pred)\n",
    "print(y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit ('3.9.1')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6ad87dd819912217535c9e765bf7b9ebff52cfacfae1ecabbf23377bfd4399d6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
