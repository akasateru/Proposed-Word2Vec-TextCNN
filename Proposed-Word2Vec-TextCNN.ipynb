{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from tensorflow import keras\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import gensim.downloader\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import string\n",
    "import csv \n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "# モデルの保存名に使用\n",
    "import datetime\n",
    "t_delta = datetime.timedelta(hours=9)\n",
    "JST = datetime.timezone(t_delta, 'JST')\n",
    "now = datetime.datetime.now(JST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "SAVED_MODEL = \"../model/Proposed-Word2Vec-TextCNN_\"+str(now.strftime('%Y%m%d%H%M%S'))\n",
    "THRESHOLD = 0.05\n",
    "MAXLEN_GET_PSEUDO = 3000\n",
    "EPOCH = 5\n",
    "BATCH_SIZE = 64\n",
    "MAX_LEN = 128\n",
    "FILTERS = 100\n",
    "FILTER_SIZES = [3,4,5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../model/Proposed-Word2Vec-TextCNN_20221129152254\n"
     ]
    }
   ],
   "source": [
    "print(SAVED_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 前処理\n",
    "def preprocessing(text):\n",
    "    # 括弧内文章の削除\n",
    "    text = re.sub(r'\\(.*\\)',' ',text)\n",
    "    text = re.sub(r'\\[.*\\]',' ',text)\n",
    "    text = re.sub(r'\\<.*\\>',' ',text)\n",
    "    text = re.sub(r'\\{.*\\}',' ',text)\n",
    "    # 記号文字の削除\n",
    "    text = text.translate(str.maketrans('','',string.punctuation))\n",
    "    # スペースの調整\n",
    "    text = re.sub(r'\\s+',' ',text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18846/18846 [00:01<00:00, 18138.03it/s]\n"
     ]
    }
   ],
   "source": [
    "# 20 newsgroups datasets\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "newsgroups = fetch_20newsgroups(subset=\"all\")\n",
    "newsgroups_datasets = list()\n",
    "\n",
    "# # example ------------------------------------------------\n",
    "# for texts in tqdm(newsgroups.data[:1000]):\n",
    "#   texts = texts.split(\"\\n\\n\")\n",
    "#   texts = \" \".join(texts[1:])\n",
    "#   newsgroups_datasets.append(preprocessing(texts))\n",
    "# # --------------------------------------------------------\n",
    "\n",
    "for texts in tqdm(newsgroups.data):\n",
    "  texts = texts.split(\"\\n\\n\")\n",
    "  texts = \" \".join(texts[1:])\n",
    "  newsgroups_datasets.append(preprocessing(texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1300000/1300000 [00:30<00:00, 43022.72it/s]\n"
     ]
    }
   ],
   "source": [
    "# yahoo topic datasets\n",
    "with open('../data/topic/train_pu_half_v0.txt','r',encoding='utf-8') as f:\n",
    "    texts_v0 = f.read()\n",
    "with open('../data/topic/train_pu_half_v1.txt','r',encoding='utf-8') as f:\n",
    "    texts_v1 = f.read()\n",
    "texts = texts_v0 + texts_v1\n",
    "topic_datasets = list()\n",
    "\n",
    "# # example ----------------------------------------------\n",
    "# for label_text in tqdm(texts.splitlines()[:1000]):\n",
    "#   _, text = label_text.split(\"\\t\")\n",
    "#   topic_datasets.append(preprocessing(text))\n",
    "# # -------------------------------------------------------\n",
    "\n",
    "for label_text in tqdm(texts.splitlines()):\n",
    "  _, text = label_text.split(\"\\t\")\n",
    "  topic_datasets.append(preprocessing(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 762027/762027 [00:29<00:00, 25928.75it/s]\n"
     ]
    }
   ],
   "source": [
    "# reuters datasets\n",
    "with open(\"../data/reuter/sourceall.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "  reuter = f.read().split(\"\\n\")[:-1]\n",
    "\n",
    "# # example -------------------\n",
    "# import random\n",
    "# reuter = random.sample(reuter, 1000)\n",
    "# #----------------------------\n",
    "\n",
    "reuters_datasets = list()\n",
    "for label_text in tqdm(reuter):\n",
    "  _, text = label_text.split(\"\\t\")\n",
    "  reuters_datasets.append(preprocessing(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 560000/560000 [00:08<00:00, 68972.62it/s]\n"
     ]
    }
   ],
   "source": [
    "# dbpedia datasets train\n",
    "with open('../data/dbpedia_csv/train.csv','r',encoding='utf-8') as f:\n",
    "    reader = [r for r in csv.reader(f)]\n",
    "    \n",
    "# # example -------------------\n",
    "# import random\n",
    "# reader = random.sample(reader, 1000)\n",
    "# #----------------------------\n",
    "\n",
    "dbpedia_train_datasets = list()\n",
    "for _, auth, text in tqdm(reader):\n",
    "    text = text.replace(auth,'')\n",
    "    dbpedia_train_datasets.append(preprocessing(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_texts = newsgroups_datasets + topic_datasets + reuters_datasets + dbpedia_train_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dbpedia classes\n",
    "with open(\"../data/dbpedia_csv/classes.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "  classes = f.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70000/70000 [00:01<00:00, 69566.00it/s]\n"
     ]
    }
   ],
   "source": [
    "# load test data\n",
    "# dbpedia datasets test\n",
    "with open('../data/dbpedia_csv/test.csv','r',encoding='utf-8') as f:\n",
    "    reader = [r for r in csv.reader(f)]\n",
    "    \n",
    "# # example -------------------\n",
    "# import random\n",
    "# reader = random.sample(reader, 1000)\n",
    "# #----------------------------\n",
    "\n",
    "test_texts = list()\n",
    "test_labels = list()\n",
    "for labels, auth, text in tqdm(reader):\n",
    "    text = text.replace(auth,'')\n",
    "    test_texts.append(preprocessing(text))\n",
    "    test_labels.append(int(labels)-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choice method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec = gensim.downloader.load('word2vec-google-news-300')\n",
    "# word2vec = gensim.models.KeyedVectors.load_word2vec_format('../data/GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 文章をベクトルに変換\n",
    "# 文章内に複数同じ単語が出現する場合、1度だけ使用する\n",
    "def w2v_avg_vector(sentence):\n",
    "  vector = np.zeros((300,), dtype=\"float32\")\n",
    "  count = 0\n",
    "  used_word = list()\n",
    "  for word in sentence.split():\n",
    "    if word not in used_word:\n",
    "      used_word.append(word)\n",
    "      try:\n",
    "        vector = np.add(vector, word2vec[word])\n",
    "        count += 1\n",
    "      except:\n",
    "        pass\n",
    "    if count >= MAX_LEN:\n",
    "      break\n",
    "  if count > 0:\n",
    "    vector = np.divide(vector, count)\n",
    "  return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes_vector = [w2v_avg_vector(cls) for cls in classes]\n",
    "\n",
    "# # # クラス名にwordnetを適用\n",
    "# classes_vector = list()\n",
    "# for cls in classes:\n",
    "#   wordnet_sequence = list()\n",
    "#   for word in cls.split():\n",
    "#     if wordnet.synsets(word):\n",
    "#       wordnet_sequence.append(wordnet.synsets(word)[0].definition())\n",
    "#   classes_vector.append(w2v_avg_vector(preprocessing(\" \".join(wordnet_sequence))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2640873/2640873 [09:56<00:00, 4424.72it/s]\n"
     ]
    }
   ],
   "source": [
    "# 情報源領域の文章と各クラスの類似度を計算し、上位2クラスの差が閾値を超えた場合、1位のクラスの学習データとする\n",
    "diff_datasets = {i:[] for i in range(len(classes))}\n",
    "for texts in tqdm(datasets_texts):\n",
    "  texts_vector = w2v_avg_vector(texts)\n",
    "  similarity = cosine_similarity([texts_vector], classes_vector)[0]\n",
    "  sim_argsorted = np.argsort(similarity)\n",
    "  diff = similarity[sim_argsorted[-1]] - similarity[sim_argsorted[-2]]\n",
    "  if diff >= THRESHOLD:\n",
    "    diff_datasets[sim_argsorted[-1]].append((diff, texts))\n",
    "\n",
    "# 上位2クラスの差が閾値を超えた文章の中で、その差が大きいものから順に各クラスの疑似ラベル付きデータの数が同じになるように選択\n",
    "pseudo_texts = list()\n",
    "pseudo_labels = list()\n",
    "for i in range(len(classes)):\n",
    "  sorted_diff_data = sorted(diff_datasets[i], reverse=True)[:MAXLEN_GET_PSEUDO]\n",
    "  pseudo_texts.extend([i[1] for i in sorted_diff_data])\n",
    "  pseudo_labels.extend([i]*len(sorted_diff_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of all selected data\n",
      "Com. : 71598\n",
      "Edu. : 36764\n",
      "Art. : 7625\n",
      "Ath. : 17758\n",
      "Off. : 86410\n",
      "Mea. : 204570\n",
      "Bui. : 9172\n",
      "Nat. : 37282\n",
      "Vil. : 34984\n",
      "Ani. : 4312\n",
      "Pla. : 13165\n",
      "Alb. : 39291\n",
      "Fil. : 23275\n",
      "Wri. : 23976\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of all selected data\")\n",
    "for i in diff_datasets:\n",
    "  print(classes[i][:3]+\". : \"+str(len(diff_datasets[i])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = keras.preprocessing.text.Tokenizer()\n",
    "tokenizer.fit_on_texts(pseudo_texts)\n",
    "tokenizer.fit_on_texts(test_texts)\n",
    "embedding_matrix = np.zeros((max(tokenizer.word_index.values())+1, word2vec.vector_size))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    if word in word2vec:\n",
    "        embedding_matrix[i] = word2vec[word]\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(pseudo_texts)\n",
    "x_train = keras.preprocessing.sequence.pad_sequences(sequences, maxlen=MAX_LEN, padding='post')\n",
    "y_train = np.array(keras.utils.to_categorical(pseudo_labels))\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(test_texts)\n",
    "x_test = keras.preprocessing.sequence.pad_sequences(sequences, maxlen=MAX_LEN, padding='post')\n",
    "y_test = np.array(keras.utils.to_categorical(test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = keras.layers.Input(shape=(MAX_LEN),dtype='float32')\n",
    "embedding = keras.layers.Embedding(input_dim=len(embedding_matrix),output_dim=len(embedding_matrix[0]),weights=[embedding_matrix],trainable=False, mask_zero=True,name='embedding')(inputs)\n",
    "conv1 = keras.layers.Conv1D(filters=FILTERS, kernel_size=FILTER_SIZES[0], kernel_initializer='normal', activation='relu')(embedding)\n",
    "conv2 = keras.layers.Conv1D(filters=FILTERS, kernel_size=FILTER_SIZES[1], kernel_initializer='normal', activation='relu')(embedding)\n",
    "conv3 = keras.layers.Conv1D(filters=FILTERS, kernel_size=FILTER_SIZES[2], kernel_initializer='normal', activation='relu')(embedding)\n",
    "pool1 = keras.layers.MaxPooling1D(pool_size=int(conv1.shape[1]),strides=1)(conv1)\n",
    "pool2 = keras.layers.MaxPooling1D(pool_size=int(conv2.shape[1]),strides=1)(conv2)\n",
    "pool3 = keras.layers.MaxPooling1D(pool_size=int(conv3.shape[1]),strides=1)(conv3)\n",
    "x = keras.layers.concatenate([pool1, pool2, pool3])\n",
    "x = keras.layers.Flatten()(x)\n",
    "x = keras.layers.Dropout(0.5)(x)\n",
    "x = keras.layers.Dense(100,activation='relu')(x)\n",
    "output = keras.layers.Dense(units=14, activation='softmax')(x)\n",
    "model = keras.models.Model(inputs, output)\n",
    "opt = keras.optimizers.Adam(learning_rate=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-07, amsgrad=True)\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "657/657 [==============================] - 45s 67ms/step - loss: 1.1858 - accuracy: 0.6908 - val_loss: 1.0818 - val_accuracy: 0.6712\n",
      "Epoch 2/5\n",
      "657/657 [==============================] - 45s 69ms/step - loss: 0.2477 - accuracy: 0.9450 - val_loss: 0.9517 - val_accuracy: 0.7024\n",
      "Epoch 3/5\n",
      "657/657 [==============================] - 45s 69ms/step - loss: 0.1339 - accuracy: 0.9690 - val_loss: 1.0651 - val_accuracy: 0.6842\n",
      "Epoch 4/5\n",
      "657/657 [==============================] - 47s 72ms/step - loss: 0.0965 - accuracy: 0.9759 - val_loss: 1.0771 - val_accuracy: 0.6868\n",
      "Epoch 5/5\n",
      "657/657 [==============================] - 47s 71ms/step - loss: 0.0765 - accuracy: 0.9805 - val_loss: 1.1014 - val_accuracy: 0.6881\n"
     ]
    }
   ],
   "source": [
    "# モデルの学習\n",
    "result = model.fit(x=x_train, y=y_train, epochs=EPOCH, batch_size=BATCH_SIZE, validation_data=(x_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _update_step_xla while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../model/Proposed-Word2Vec-TextCNN_20221129152254\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../model/Proposed-Word2Vec-TextCNN_20221129152254\\assets\n"
     ]
    }
   ],
   "source": [
    "model.save(SAVED_MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model(SAVED_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2188/2188 [==============================] - 24s 11ms/step\n"
     ]
    }
   ],
   "source": [
    "pred = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Com.      0.875     0.529     0.659      5000\n",
      "        Edu.      0.736     0.869     0.797      5000\n",
      "        Art.      0.819     0.531     0.644      5000\n",
      "        Ath.      0.965     0.840     0.898      5000\n",
      "        Off.      0.647     0.818     0.723      5000\n",
      "        Mea.      0.660     0.490     0.563      5000\n",
      "        Bui.      0.686     0.676     0.681      5000\n",
      "        Nat.      0.333     0.902     0.487      5000\n",
      "        Vil.      0.906     0.959     0.932      5000\n",
      "        Ani.      0.700     0.170     0.274      5000\n",
      "        Pla.      0.752     0.409     0.529      5000\n",
      "        Alb.      0.894     0.948     0.920      5000\n",
      "        Fil.      0.862     0.736     0.794      5000\n",
      "        Wri.      0.611     0.756     0.676      5000\n",
      "\n",
      "    accuracy                          0.688     70000\n",
      "   macro avg      0.746     0.688     0.684     70000\n",
      "weighted avg      0.746     0.688     0.684     70000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_test = [np.argmax(i) for i in y_test]\n",
    "y_pred = [np.argmax(i) for i in pred]\n",
    "\n",
    "target_names = [c[:3]+\".\" for c in classes]\n",
    "\n",
    "rep = metrics.classification_report(y_test, y_pred, target_names=target_names, digits=3)\n",
    "print(rep)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9e32349538976b425bbf8209bec3a52ef38eb988b8b568d4d0fb100f86dbbec2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
